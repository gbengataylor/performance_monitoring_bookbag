:guid: %guid%
:user: %user%
:openshift_user_password: %openshift_user_password%
:openshift_console_url: %openshift_console_url%
:user_devworkspace_url: %user_devworkspace_url%
:template-github-url: %template-github-url%
:hyperfoil_web_cli_url: %hyperfoil_web_cli_url%
:hyperfoil_benchmark_definition_url: 'https://raw.githubusercontent.com/redhat-na-ssa/workshop_performance-monitoring-apps-template/main/scripts/hyperfoil/summit-load-apps.hf.yaml'
:grafana_url: %grafana_url%
:markup-in-source: verbatim,attributes,quotes
:source-highlighter: highlight.js

[[scaling]]
= Load Testing, Scaling and Monitoring the Applications

Our microservices have several endpoints consuming memory or CPU.
Depending on the number of iterations and bites we pass as parameters to these endpoints, it can take significantly longer to respond.
We are going to explore the problem by looking at *CPU* and *memory* metrics.

== CPU

When you know CPU usage you are better prepared to answer the following questions:

* Is the amount of CPU resources maxed out?
* Have I over provisioned the amount of CPU resources?
* What does the baseline of usage looks like?
* Is there room to grow without scaling out or up?
* How much of the available CPU resources is it really using?
* What type of load is it?

== Memory

When you know memory usage you are better prepared to answer the following questions:

* Is the amount of memory used close to the maximum available memory?
* Have I over provisioned the amount of memory resources?
* What does the baseline of usage look like?
* Is there room to grow without scaling out or up?
* How much of the available memory resources is it really using?

== Monitoring

One of the first things you usually want to do once your application is deployed is to configure monitoring.
For this, we'll be using the Monitoring capability built in Openshift. This feature comes with a pre-set of ready to use dashboards for monitoring application's workload metrics right from the Openshift Console. 

[NOTE]
====
The OpenShift Container Platform monitoring stack is based on the link:https://prometheus.io/[Prometheus] open source project and its wider ecosystem. To learn more about the Openshift Monitoring stack see link:https://docs.openshift.com/container-platform/4.12/monitoring/monitoring-overview.html[About OpenShift Container Platform monitoring].
====

We'll be also using the link:https://grafan.com[Grafana] tool to monitor some specific metrics provided out-of-the-box by the Openshift Monitoring Stack.

[NOTE]
====
Grafana is an open platform for beautiful analytics and monitoring. For more information please visit the link:https://grafana.com/oss/[Grafana website].

Red Hat provides an Openshift Operator to install and manage Grafana instances on your cluster. See link:https://operatorhub.io/operator/grafana-operator[Grafana Operator] in the Operator Hub for more details.
====

From the link:{openshif_console_url}/topology/ns/{user}-staging?view=graph[Developer Perspective in Openshift Console] navigate to the Observe view. 
From there, select the *Dashboard* `Kubernetes / Compute Resources / Namespaces (Workloads)` and *Type* `Deployment` for instance.

image::./imgs/module-5/ocp_console_observe_dashboards.gif[Screenshot of the Openshift Developer perspective - Observe - Dashboad view]

You can also drill into the Metric Graph by clicking in Inspec link (top right of the graph). This will open the metric view where you can
see the link:https://prometheus.io/docs/prometheus/latest/querying/basics/[PromoQL^] that brings this metric data point from the integrated Prometheus Data Source.

image::./imgs/module-5/ocp_console_observe_metrics.gif[Screenshot of the Openshift Developer perspective - Observe - Metrics view]

Using the *Dashboard* and *Metric* views, you can select which metrics you want to observe, and the time range for the data.
The *Observe* view is contextual which means it will present the data based on the project namespace you are in. So, you can observe the metrics for the workloads you have deployed into the current project namespace.
If you switch to a different namespace you should see the workloads deployed to that specific project namespace.

[TIP]
====
When you have multiple deployments (workloads) running in the current namespace you can notice that these graphs aggregates the data-points by POD. 
Take a look at the PromoQL of the following CPU metric graph.

image::./imgs/module-5/ocp_console_observe_aggregated_metrics_cpu.png[Aggregated CPU metrics]
====

=== Using Grafana to Visualize Metrics

Now lets look at the application metrics using a different tool that can be easily integrated with Openshift.
Because Openshift already uses Prometheus to gather and store cluster and application metrics you can easily integrate Grafana to create 
nice metric visualization dashboards for any workload running on Openshift.

Our Lab already has a Grafana instance fully integrated with Openshift's Monitoring stack. You can access Grafana by link:{grafana_url}[clicking here^]
Then click on the *General / Home* link at the top left then click on *grafana | Go to folder* and select *Kubernetes / Compute Resources / Namespace (Workloads)*
At the top of this dashboard you can select your *namespace* ({user}-staging) and 'deployment' *type*.

image::./imgs/module-5/grafana_workload_dashboards.gif[Grafana workload dashboard]

This is the same data we just saw using the Openshift Observe view in the Developer Console, but now using a different visualization tool. 

Grafana is a very powerful metric visualization tool that allows you to create rich dashboards to monitor you app and get data insights from your application metrics.
Creating customized dashboards is outside the scope of our lab but you can explore a vast library of tutorials on link:https://grafana.com/tutorials[Grafana website^].

[IMPORTANT]
====
It's important to notice that the metrics' data source (Prometheus) is the same regardless the visualization tool you are using.
====

== Logs

The http://portal.azure.com[Azure Portal] also provides a nice interface to view the logs of your application.
This is crucial to troubleshoot issues when something goes wrong.

You have multiple options to view the different logs of your application:

- You can connect to a given container apps instance and gets the stream of console logs.
This is useful to troubleshoot issues with a specific instance of your application in real time.

- You can also access the console logs in http://portal.azure.com[Azure Portal].
You get access to all logs from the https://learn.microsoft.com/azure/azure-monitor/logs/log-analytics-overview[Log Analytics] workspace we created earlier.
Using SQL-like queries (called https://learn.microsoft.com/azure/data-explorer/kusto/query[Kusto Query Language], or KQL), you can filter the logs and get the information you need across all your applications, revisions, and instances.

- Finally, you also have access to the _system logs_, which are the logs of the container host.
It's very useful to troubleshoot issues with the container host itself and find out why your container is not running.

=== Streaming logs of a container instance

The most straightforward way to access the logs of your application is to connect to a given container instance and get the stream of console logs.
You can directly access the stream of logs from the latest revision of a running instance of your application with this command:

[source,shell]
----
 az containerapp logs show \
  --name "$QUARKUS_APP" \
  --resource-group "$RESOURCE_GROUP" \
  --format text \
  --follow
----

[TIP]
====
Don't forget that by default, containers apps scale out to 0 instances when they are not used.
You can use the command `curl https://${QUARKUS_HOST}/quarkus` to wake up the container app first if the connections fails.
====

Once you're connected, you can see the logs of your application in real time.
The `--follow` option keeps the connection open to see the new logs in real time.

In a different terminal, you can use the `curl` command to generate some load on the application.
You should see the logs of the application being updated in real time.

[source,shell]
----
curl https://${QUARKUS_HOST}/quarkus/cpu?iterations=10
----

=== Viewing the console logs in Azure portal

You can also access the console logs in the http://portal.azure.com[Azure Portal] or via CLI.
Using https://learn.microsoft.com/azure/azure-monitor/logs/log-analytics-overview[Log Analytics], you get access to all logs from all your applications, revisions, and instances.
This means you can troubleshoot issues across all your applications, tracing requests as needed, which is crucial if your application use a microservices architecture.

Open the http://portal.azure.com[Azure Portal] and navigate to the resource group `rg-java-runtimes` you created for your application.
Select the `logs-java-runtimes` container app, then select *Logs* from the left menu, under the *General* group.

By default, you are presented a list of pre-defined queries.
Close this panel by clicking on the *X* button on the top right corner, as we'll create our own query.

image::../assets/log-analytics.png[Screenshot of the Azure portal showing the logs analytics panel]

Log analytics queries use the https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language[Kusto query language], which is a SQL-like language.
You can use the query language to filter the logs and get the information you need.

Let's start by creating a query to get the logs of the `quarkus-app` container app.
Enter this query in the editor:

[source,sql]
----
ContainerAppConsoleLogs_CL
| where RevisionName_s == "quarkus-app--<REVISION_ID>"
----

You can get the app revision name by running the following command:

[source,shell]
----
az containerapp revision list \
  --name "$QUARKUS_APP" \
  --resource-group "$RESOURCE_GROUP" \
  --query "[0].name" --output tsv
----

Select *Run* to execute the query.
You should see the logs of the `quarkus-app` container app.

image::../assets/log-analytics-query.png[Screenshot of the logs results in Azure portal]

For now, it's not very useful as it's the same logs we saw in the previous section.
Let's add some filters to the query to search for error messages from the logs:

[source,sql]
----
ContainerAppConsoleLogs_CL
| where RevisionName_s == "quarkus-app--<REVISION_ID>"
| where Log_s !has "INFO"
| where Log_s contains "error"
----

Select *Run* to execute the query.
If your application is working fine, you should not see any results.
Let's generate some errors by crashing the application with the following command:

[source,shell]
----
curl https://${QUARKUS_HOST}/quarkus/memory?bites=1000
----

Oops! We're trying to allocate more memory than the container has available, resulting in a crash because of our (crude) memory allocation algorithm.

If you run the query again, you should see the error message in the logs:

image::../assets/log-analytics-error.png[Screenshot of the logs results in Azure portal]

[TIP]
====
Of course, you can go much further with the query language.
You can have a look at the https://learn.microsoft.com/en-us/azure/data-explorer/kql-quick-reference[quick reference] to play a bit with the queries.
====

We can make it easier to read by making the latest logs appear at the top of the results, and only show the time and message of the last 10 logs:

[source,sql]
----
ContainerAppConsoleLogs_CL
| where RevisionName_s == "quarkus-app--<REVISION_ID>"
| where Log_s !has "INFO"
| where Log_s contains "error"
| project TimeGenerated, Log_s
| sort by TimeGenerated desc
| take 10
----

And here we can quickly see our `OutOfMemoryError` error message.

image::../assets/log-analytics-error-2.png[Screenshot of the logs results in Azure portal]

We can save the query for later use by clicking on the *Save* button.
You can then give it a name and a description, and a category to quickly find it later.

=== Viewing the console logs with the CLI

You can also view the logs via a CLI command instead of using the portal.
It's just a matter of writing any KQL query in the `analytics-query` parameter as follow:

[source,shell]
----
az monitor log-analytics query \
--workspace $LOG_ANALYTICS_WORKSPACE_CLIENT_ID \
--analytics-query "ContainerAppConsoleLogs_CL | where RevisionName_s == 'quarkus-app--<REVISION_ID>'" \
--output table
----

=== Viewing the system logs

While console logs are very useful to troubleshoot issues with your application, it won't be much help if your container is not running.
In this case, you need to troubleshoot the container host itself.
This is where the _system logs_ come in handy.

The system logs are the logs of the container host.
They are very useful to troubleshoot issues with the container host itself and monitor its activity during provisioning operations.

System logs can be accessed the same way as console logs.
Change the previous query in the editor to get the system logs:

[source,sql]
----
ContainerAppSystemLogs_CL
| where RevisionName_s == "quarkus-app--<REVISION_ID>"
| project TimeGenerated, Reason_s, Log_s
| sort by TimeGenerated desc
----

You should see the system logs of the `quarkus-app` container app, covering the whole lifecycle of the container.

image::../assets/log-analytics-system.png[Screenshot of the logs results in Azure portal]

Using the CLI you can execute:

[source,shell]
----
az monitor log-analytics query \
--workspace $LOG_ANALYTICS_WORKSPACE_CLIENT_ID \
--analytics-query "ContainerAppSystemLogs_CL | where RevisionName_s == 'quarkus-app--<REVISION_ID>' | project TimeGenerated, Reason_s, Log_s | sort by TimeGenerated desc" \
--output table
----

== Load Testing

Now it's time to add some load to our app services.
For the purpose of our Lab this will allow us to see two things:

1. observe resource consumption metrics (CPU, memory, networking traffic, etc) for each app container.
2. how the auto-scaling features works in Openshift.

== Introducing Hyperfoil

Nowadays there are many opensource tools that can be used to generate application load towards your applications. 
Many of them was designed before Containers and Kubernetes making its deployment, configuration and scalability a challenge considering the distributed nature of Kubernetes. 

For this Lab we will be using a tool called link:https://hyperfoil.io[Hyperfoil.io^]. Hyperfoil is described as a *Microservice-oriented distributed benchmark framework*.
Because Hyperfoil is kubernetes-native (built to run on a scalable containerized distributed environment) it's perfect for our use-case as it is easy to deploy and run on any Kubernetes environment.

[TIP]
====
Hyperfoil is a powerful and flexible (kubernetes-native) *Load Driver* tool that can be applied to very robust use-cases. It was initially developed by the Red Hat Engineering team to test
our own Software but is now available as an link:https://github.com/Hyperfoil/Hyperfoil[opensource project^] for anyone to use and contribute.

To see more details around the motivation and purpose of the tool, please see its link:https://hyperfoil.io/docs[Documentation here^]
====

=== Hyperfoil Architecture
As mentioned early in its nature Hyperfoil is a distributed tool with link:https://martinfowler.com/articles/patterns-of-distributed-systems/leader-follower.html[leader-follower architecture^]. 
Controller has the leader role; this is a link:https://vertx.io/[Vert.x-based^] server with REST API. 
When a benchmark is started controller deploys agents (according to the benchmark definition), pushes the benchmark definition to these agents and orchestrates benchmark phases. 
Agents execute the benchmark, periodically sending statistics to the controller. 
This way the controller can combine and evaluate statistics from all agents on the fly. When the benchmark is completed all agents terminate.

image::./imgs/module-5/hyperfoil-diagram.png

In order to run a load testing using Hyperfoil we need to create a link:https://hyperfoil.io/userguide/benchmark.html[Benchmark^]. In Hyperfoil terms a Benchmark is a declarative YAML file where you define
your load test Scenario. Lets take a look at one snippet of the 
link:https://github.com/redhat-na-ssa/workshop_performance-monitoring-apps-template/blob/main/scripts/hyperfoil/summit-load-apps.hf.yaml[benchmark definition] 
we'll be using thought this section of our Lab.

[source, yaml]
----
# This is the name of the benchmark. It's recommended to keep this in sync with
# name of this file, adding extension `.hf.yaml`.
name: summit-lab-load-apps
# We must define at least one HTTP target, in this case it becomes a default
# for all HTTP requests.
http:
- host: http://quarkus-app:8080

# Distribute the connections among two agents
agents:
  agent-01:
    node: kubernetes.io/os=linux # you can use labels to select nodes
  #... define many agents as you may need

# Simulation consists of phases - potentially independent workloads.
# all phases are started independently at the same moment (when the benchmark was started)
phases:
# `wakeUp` is the name of the 1st phase in this benchmark.
- wakeUp:
    # The benchmark will start certain number of users according to a schedule 
    # regardless of previously started users completing the scenario. 
    # This is the open-model
    atOnce:
      users: 1 # run the scenario exactly once
      scenario:
      - serviceWakeup: # the only sequence for this scenario
        # In the only step in this sequence we'll do a HTTP GET request
        # Steps are the basic building blocks that form each sequence of a scenario, 
        # similar to statements in a programming language. Steps are potentially blocking (the sequence cannot continue with next step until previous one finishes).
        - httpRequest:
            GET: /quarkus
            # Inject helpers to make this request synchronous, i.e. keep
            # the sequence blocked until Hyperfoil processes the response.
            sync: true

# 2nd phase of this bechmark
- hello:
    constantRate:
      startAfterStrict: wakeUp # will wait until the wakeUp phase is TERMINATED (all requests done!)
      # 10 users will be starting the scenario every second (during 120s)
      usersPerSec: 10 # run the scenario 'n' times per second in average
      duration: 180s #let it run for about 3min...
      # In Hyperfoil, everything is pre-allocated = limited in size. Here we'll
      # set that we won't run more than LOOP (1 by default) iterations of this phase.
      # maxIterations: !param LOOP 1
      scenario:
      - sayHello: #sequence
        - httpRequest:
            GET: !param APP_CONTEXT_PATH
            endpoint: app-host
        # Wait 3 seconds to simulate user-interaction
        - thinkTime:
            duration: 300 ms
#...
----

The benchmark used in our lab defines the following structure:

 * 3 agents
 * A scenario with 6 phases

  1. wakeUp
  2. hello
  3. cpuRampUp
  4. cpuWithPersistence
  5. memoryRampUp
  6. memoryWithPersistence

[NOTE]
====
The full benchmark definition can be found inside the lab repo: `$PROJECT_SOURCE/main/scripts/hyperfoil/summit-load-apps.hf.yaml`. 
====

[TIP]
====
The inline comments in the above benchmark snippet give us a glimpse of what each property means. 
To better understand the basic concepts of Hyperfoil we recommend reading though the following sections of the in the Project Documentation:

 * link:https://hyperfoil.io/quickstart/quickstart1.html[Getting started: First benchmark]
 * link:https://hyperfoil.io/docs/concepts.html[Concepts]
 * link:https://hyperfoil.io/userguide/benchmark.html[Benchmark definition format]
 * link:https://hyperfoil.io/docs/reference_index.html[steps and handlers used in a scenario]
 * link:https://hyperfoil.io/userguide/examples.html[Complex Benchmark definition samples]
====


=== Running a Hyperfoil Benchmark

Once you have a Benchmark definition you are ready to run it using Hyperfoil Controller. We already deployed one Hyperfoil instance for you
in  the `{user}-hyperfoil` project namespace for you. You can access it using its link:{hyperfoil_web_cli_url}[Web CLI]. 

image::./imgs/module-5/hyperfoil-web-cli-open.gif[Screenshot of Hyperfoil Web CLI]

From the Web CLI you can upload our benchmark and start running it to generate load towards your apps.
Follow the following steps:

1.
Inside the Web CLI type `upload` and hit `Enter`. 

2.
Click inside the `Input Text` field that appears in the console.

3. 
Paste this URL: `{hyperfoil_benchmark_definition_url}` and click the `Upload from URL` button.

image::./imgs/module-5/hyperfoil-web-cli-upload.gif[Screenshot of Hyperfoil Web CLI - uploading a benchmark definition]

Once you have the benchmark file uploaded to the Controller you can start your first `run` by just executing 

[source, shell, role=copy]
----
run summit-lab-load-apps
----

At the first `run` you will be prompted two `params` (defined inside the benchmark file by using the `!param` notation).

 * the first one is the context path of the application (`/quarkus` for the `quakus-app`)
 * the second one is the application URI. You can use the internal Kubernetes cluster service URI or the Openshift external Ingress Route 

For instance, to start a `run` for the `quarkus-app` enter the param values as follows:

[source, shell, role=copy]
----
run summit-lab-load-apps
APP_CONTEXT_PATH=/quarkus
APP_URL=http://quarkus-app.{user}-staging.svc.cluster.local
----

[TIP]
====
 * to get the correct value for `APP_URL` param go to you DevWorkspace adn from the Task Manager execute the Task `10: Show Hyperfoil and Apps Routes`.
 * you don't if needed to make any change to the bechmark definition.
 But if you need to (or just want to see its definition) you can open it right from the Web CLI using an embedded editor. To do that just type `edit summit-lab-load-apps` in the Web CLI as shown in the screenshot. 
====

The screenshot bellow show how to start a benchmark `run` agains the  `quarkus-app`.

image::./imgs/module-5/hyperfoil-web-cli-run.gif[Screenshot of Hyperfoil Web CLI - running a benchmark against the quarkus-app]

Now that you know how to execute your benchmark inside the Hyperfoil Web CLI, run it against the `micronaut-app` and the `springboot-app`.
This time When executing a new run you *have to* explicitly pass the `APP_CONTEXT_PATH` and the `APP_URL` to the `run` command, 
otherwise it will use the value you entered the first time. 

To run against the `micronaut-app` use:

[source, shell, role=copy]
----
run summit-lab-load-apps -PAPP_URL=http://micronaut-app.{user}-staging.svc.cluster.local -PAPP_CONTEXT_PATH=/micronaut
----

To run against the `springboot-app` use:

[source, shell, role=copy]
----
run summit-lab-load-apps -PAPP_URL=http://springboot-app.{user}-staging.svc.cluster.local -PAPP_CONTEXT_PATH=/springboot
----

[NOTE]
====
By default each `run` will last for ~2min (`PHASE_DURATION_SECS` param default to `120s`).
====

[TIP]
====
Other parameters are defined in our benchmark definition. They can be used to adjust how Hyperfoil should run a benchmark.

[source,properties]
----
APP_URL (required!)
APP_CONTEXT_PATH (required!)
SHARED_CONN (default to 200)
USERS_PER_SEC (default to 10)
PHASE_DURATION_SECS (default to 120s)
CPU_ITERATIONS (default to 5)
MEMORY_BITES (default 20)
----
====

=== Running the tests

After the test creation, it will start automatically after a short time.
When the test run finishes, you will get some metrics:

image::../assets/load-testing-metrics.png[Screenshot of test run]

In the http://portal.azure.com[Azure Portal] sidebar, select *Dashboards* and go back to the "_Java Runtimes_" dashboard we created earlier.

[TIP]
====
There might be a slight delay before the metrics are visible in the dashboard.
====

If you take a look at the charts, you can see CPU and memory usage increase, and also that the number of replicas has increased from 1 replica to 10.
Azure Container Apps has scaled automatically the application depending on the load.

image::../assets/dashboard-scale.png[Screenshot of dashboard showing load testing results]

== Scaling

Now that we have seen the auto-scaling in action, let's dive a bit into fine-tuning its configuration.

[NOTE]
====
Azure Container Apps only support automatic _horizontal_ scaling, meaning that it will only scale the number of replicas of your application.
Vertical scaling (increasing the amount of available CPU and memory) is supported, but you need to do it manually.
====

Azure Container Apps support different types of https://learn.microsoft.com/azure/container-apps/scale-app[scaling rules], implemented using the KEDA https://keda.sh/docs/concepts/scaling-deployments/#scaledobject-spec[Scaling Object].
It supports the following triggers:

- _HTTP traffic_: Scaling based on the number of concurrent HTTP requests to your revision.
This is the default scaling rule.
- _TCP traffic_: Scaling based on the number of concurrent TCP requests to your revision.
- _Event-driven_: Event-based triggers such as messages in an Azure Service Bus.
- _CPU_ or _Memory usage_: Scaling based on the amount of CPU or memory consumed by a replica.

[NOTE]
====
By default, when you deploy a container app, it is set to scale from 0 to 10 replicas. The default scaling rule uses HTTP scaling and defaults to a minimum of 10 concurrent requests per second.
====

As our applications provides endpoints to load either the CPU or the memory, we will explore usage of the _CPU_ and _Memory usage_ triggers to scale our application.

=== Scaling based on CPU usage

To scale based on CPU usage, we need to update the scale rule of the application to use the `cpu` trigger.
This will create a new revision of the application (but the URL `$QUARKUS_HOST` remains unchanged), and will start a new deployment.

We will set a new scale rule for our Quarkus app using the Azure CLI:

[source,shell]
----
include::{project-root}/scripts/infra/scale.sh[tag=adocAutoScalingCpu, indent=0]
----

This will automatically scale out the application when the CPU usage is above 10% (we set it low deliberately to make it easy to go up).

image::../assets/dashboard-scale-setcpu.png[Screenshot of setting up CPU scaler]

Go back to the http://portal.azure.com[Azure Portal], and search for `lt-java-runtimes` to open again our load testing instance.
Select *Tests* in the left sidebar, open the test we created earlier and select *Run* to run the load tests again.

Once the test is finished, go back to the "_Java Runtimes_" dashboard and take a look at the number of replicas chart again.
You should see that the number of replicas has increased to 10, and that the CPU usage has increased as well.

image::../assets/dashboard-scale-cpu.png[Screenshot of dashboard showing CPU scaler results]

[NOTE]
====
When using either the _CPU_ or _Memory usage_ triggers, the minimum number of replicas will always be 1.
Use HTTP or event based triggers to allow scaling to 0.
====

=== Scaling based on memory usage

Another option that we can use is to scale based on the memory usage, with the `memory` trigger.

This we will set the scale rule for our Micronaut app using the command:

[source,shell]
----
include::{project-root}/scripts/infra/scale.sh[tag=adocAutoScalingMemory, indent=0]
----

This will automatically scale out the application when the memory usage is above 15% (we set it low deliberately to make it easy to go up).

image::../assets/dashboard-scale-setmemory.png[Screenshot of setting up memory scaler]

Again, go back to the http://portal.azure.com[Azure Portal] and run the load tests again.
Open the dashboard when the test is finished, and take a look at the number of replicas chart again.

You can now compare how the CPU (Quarkus), memory (Micronaut), and HTTP (Spring) triggers behave when scaling the application, under the same load.

image::../assets/dashboard-scale-memory.png[Screenshot of dashboard showing CPU scaler results]

As you can see, using different scaling triggers allows to tune the scaling behavior of your application, depending on the type of load you want to handle.
Note that you're not limited to only one scaling trigger, you can use multiple triggers at the same time.

[TIP]
====
Fine tuning the scaling rules is a key factor to get the best performance/cost ratio for your application.
You want to make sure that you don't scale too early, and that you don't scale too much to avoid paying for resources that are not needed.
====

== Checking the Metrics in the Database

Remember that we have a PostgreSQL Database with three tables where we store our metrics.
You can execute the following SQL statements so you get all the metrics for Quarkus, Micronaut and Spring Boot.

[source,shell]
----
az postgres flexible-server execute \
  --name "$POSTGRES_DB" \
  --admin-user "$POSTGRES_DB_ADMIN" \
  --admin-password "$POSTGRES_DB_PWD" \
  --database-name "$POSTGRES_DB_SCHEMA" \
  --querytext "select Duration, Parameter, Description from Statistics_Quarkus" \
  --output table

az postgres flexible-server execute \
  --name "$POSTGRES_DB" \
  --admin-user "$POSTGRES_DB_ADMIN" \
  --admin-password "$POSTGRES_DB_PWD" \
  --database-name "$POSTGRES_DB_SCHEMA" \
  --querytext "select Duration, Parameter, Description from Statistics_Micronaut" \
  --output table

az postgres flexible-server execute \
  --name "$POSTGRES_DB" \
  --admin-user "$POSTGRES_DB_ADMIN" \
  --admin-password "$POSTGRES_DB_PWD" \
  --database-name "$POSTGRES_DB_SCHEMA" \
  --querytext "select Duration, Parameter, Description from Statistics_Springboot" \
  --output table
----
